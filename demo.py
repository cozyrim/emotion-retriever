import os
import numpy as np
import pandas as pd
import torch
from PIL import Image
import gradio as gr
from tqdm import tqdm
from transformers import DistilBertTokenizer
from config import CFG
from model import CLIPModel
from utils import load_model
from dataset import get_transforms

# 1) ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = CLIPModel()
model = load_model(model, "model_final.pth", device=CFG.device)
model.to(CFG.device).eval()
tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_encoder_model)

# 2) ì„ë² ë”© ìºì‹œ ë¡œë“œ/ìƒì„±
CACHE_DIR = "cache"
EMB_FILE = os.path.join(CACHE_DIR, "image_embeddings.npy")
IMG_FILE = os.path.join(CACHE_DIR, "image_filenames.csv")
if os.path.exists(EMB_FILE) and os.path.exists(IMG_FILE):
    image_embeddings = np.load(EMB_FILE)
    unique_imgs = pd.read_csv(IMG_FILE)
else:
    df = pd.read_csv(CFG.captions_path)
    unique_imgs = df[['image']].drop_duplicates().reset_index(drop=True)
    embs = []
    for fn in tqdm(unique_imgs['image'], desc="Embedding Images"): # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì™€ ì „ì²˜ë¦¬
        img = Image.open(os.path.join(CFG.image_path, fn)).convert("RGB")
        arr = np.array(img)
        tensor = (
            torch.tensor(get_transforms(train=False)(image=arr)["image"])
            .permute(2,0,1)
            .unsqueeze(0)
            .float()
            .to(CFG.device)
        )
        with torch.no_grad(): # ì´ë¯¸ì§€ feature ë½‘ê¸° (ì¶”ë¡ ëª¨ë“œ)
            feat = model.image_encoder(tensor)
            embs.append(model.image_proj(feat).cpu().numpy().flatten())
    image_embeddings = np.vstack(embs)
    os.makedirs(CACHE_DIR, exist_ok=True)
    np.save(EMB_FILE, image_embeddings)
    unique_imgs.to_csv(IMG_FILE, index=False)

# 3) ê²€ìƒ‰ í•¨ìˆ˜ (Top-6)
def search(caption: str):
    enc = tokenizer( # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        caption,
        padding="max_length", truncation=True,
        max_length=CFG.max_length,
        return_tensors="pt"
    )
    with torch.no_grad(): # í…ìŠ¤íŠ¸ ì„ë² ë”© ë§Œë“¤ê¸°
        txt_feat = model.text_encoder(
            enc.input_ids.to(CFG.device),
            enc.attention_mask.to(CFG.device)
        )
        txt_emb = model.text_proj(txt_feat).cpu().numpy().flatten() # ì´ë¯¸ì§€ ì„ë² ë”©ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
    sims = image_embeddings @ txt_emb / (
        np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(txt_emb)
    )
    idxs = np.argsort(-sims)[:6] # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ Top-6 ì´ë¯¸ì§€ ì„ íƒ
    return [
        Image.open(os.path.join(CFG.image_path, unique_imgs['image'][i])) # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ì—´ê¸°
        for i in idxs
    ]

# 4) Blocks ë ˆì´ì•„ì›ƒ + ì»¤ìŠ¤í…€ CSS
custom_css = """
/* â”€ ì „ì²´ ì¤‘ì•™ ì •ë ¬ & ìµœëŒ€ í­ ì œí•œ â”€ */
.gradio-container {
  max-width: 900px !important;
  margin: 0 auto;
}

/* â”€ ì œëª© í¬ê²Œ â”€ */
#title {
  font-size: 10rem !important;
  text-align: center;
  margin-top: 1rem;
  margin-bottom: 0.5rem;
}

/* â”€ ìº¡ì…˜+ë²„íŠ¼ ì„¸ë¡œ ìŠ¤íƒ, ê°€ìš´ë° ì •ë ¬ â”€ */
#search-col {
  display: flex;
  flex-direction: column;
  align-items: center;
  margin-top: 2rem;
  margin-bottom: 1.5rem;
}
/* ë²„íŠ¼ ì‚´ì§ ì•„ë˜ë¡œ */
#search-btn { margin-top: 12px; }

/* ì…ë ¥ì°½ ë‘¥ê¸€ê²Œ + ê·¸ë¦¼ì */
#txt-input textarea {
  border-radius: 8px;
  box-shadow: 0 2px 6px rgba(0,0,0,0.05);
}

/* ë²„íŠ¼ ë‘¥ê¸€ê²Œ + ê·¸ë¦¼ì */
#search-btn {
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0,0,0,0.1);
}

/* â”€ ê°¤ëŸ¬ë¦¬ 3ì—´Ã—2í–‰ â”€ */
#gallery {
  height: 480px;          /* 2í–‰ ë¶„ëŸ‰ ë†’ì´ ê³ ì • */
  overflow-y: hidden;     /* ìŠ¤í¬ë¡¤ ì—†ì´ ëª¨ë‘ ë³´ì´ë„ë¡ */
}
#gallery .wrap {
  padding: 6px;
}
#gallery img {
  border-radius: 8px;
  object-fit: cover;
  box-shadow: 0 4px 12px rgba(0,0,0,0.1);
}

/* â”€ Footer ì¤‘ì•™ ì •ë ¬ â”€ */
#footer {
  text-align: center;
  color: #888;
  margin-top: 1.5rem;
}
"""

with gr.Blocks(theme="gstaff/xkcd", css=custom_css) as demo:
    gr.Markdown("ğŸ­ **í‘œì • ì„¤ëª… ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰**", elem_id="title")

    # ìº¡ì…˜ ì…ë ¥ ë°•ìŠ¤ ì•„ë˜ì— ê²€ìƒ‰ ë²„íŠ¼ ì„¸ë¡œ ìŠ¤íƒ
    with gr.Column(elem_id="search-col"):
        txt = gr.Textbox(
            label="Caption ì…ë ¥",
            placeholder="ì˜ˆ: happy smiling face",
            lines=2,
            elem_id="txt-input"
        )
        btn = gr.Button(
            "ê²€ìƒ‰",
            variant="primary",
            elem_id="search-btn"
        )

    # 3ì—´Ã—2í–‰ ê°¤ëŸ¬ë¦¬, elem_id ì§€ì •
    gallery = gr.Gallery(
        label="ê²€ìƒ‰ ê²°ê³¼",
        columns=3,
        elem_id="gallery"
    )
    btn.click(search, inputs=txt, outputs=gallery)

    gr.Markdown("Powered by CLIP & DistilBERT", elem_id="footer")

if __name__ == "__main__":
    demo.launch()
